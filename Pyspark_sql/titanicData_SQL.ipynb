{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/11 15:38:41 WARN Utils: Your hostname, Subh-Ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.29.54 instead (on interface wlp4s0)\n",
      "22/02/11 15:38:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/11 15:38:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('titanic_data_SQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.csv(\"/home/subh/CFP/Pyspark/titanic.csv\",header= True, inferSchema= True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------------------------+--------+\n",
      "|PassengerID|Name                                               |Survived|\n",
      "+-----------+---------------------------------------------------+--------+\n",
      "|1          |Braund, Mr. Owen Harris                            |0       |\n",
      "|2          |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|1       |\n",
      "|3          |Heikkinen, Miss. Laina                             |1       |\n",
      "|4          |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |1       |\n",
      "|5          |Allen, Mr. William Henry                           |0       |\n",
      "+-----------+---------------------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to perform sql functions we create a temporary view\n",
    "df.createOrReplaceTempView('titanic')\n",
    "\n",
    "df2 = spark.sql(\"select PassengerID, Name, Survived from titanic limit(5)\").show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|   Sex|   average_survived|\n",
      "+------+-------------------+\n",
      "|female| 0.7420382165605095|\n",
      "|  male|0.18890814558058924|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"select Sex,avg(Survived) as average_survived from titanic group by Sex\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------+\n",
      "|      average_Fare|Pclass|Embarked|\n",
      "+------------------+------+--------+\n",
      "|11.214083333333337|     3|       C|\n",
      "|25.358335294117644|     2|       C|\n",
      "|              90.0|     1|       Q|\n",
      "|11.183393055555557|     3|       Q|\n",
      "|             12.35|     2|       Q|\n",
      "|104.71852941176469|     1|       C|\n",
      "| 70.36486220472443|     1|       S|\n",
      "| 14.64408300283288|     3|       S|\n",
      "|20.327439024390245|     2|       S|\n",
      "|              80.0|     1|    null|\n",
      "+------------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"select avg(Fare) as average_Fare, Pclass, Embarked from titanic group by Pclass,Embarked\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Pclass|count|\n",
      "+------+-----+\n",
      "|     1|  216|\n",
      "|     3|  491|\n",
      "|     2|  184|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.groupBy('Pclass').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+-----------+--------+------+\n",
      "|Name                                               |PassengerID|Fare    |Pclass|\n",
      "+---------------------------------------------------+-----------+--------+------+\n",
      "|Cumings, Mrs. John Bradley (Florence Briggs Thayer)|2          |71.2833 |1     |\n",
      "|Futrelle, Mrs. Jacques Heath (Lily May Peel)       |4          |53.1    |1     |\n",
      "|McCarthy, Mr. Timothy J                            |7          |51.8625 |1     |\n",
      "|Fortune, Mr. Charles Alexander                     |28         |263.0   |1     |\n",
      "|Spencer, Mrs. William Augustus (Marie Eugenie)     |32         |146.5208|1     |\n",
      "|Meyer, Mr. Edgar Joseph                            |35         |82.1708 |1     |\n",
      "|Holverson, Mr. Alexander Oskar                     |36         |52.0    |1     |\n",
      "|Laroche, Miss. Simonne Marie Anne Andree           |44         |41.5792 |2     |\n",
      "|Harper, Mrs. Henry Sleeper (Myna Haxtun)           |53         |76.7292 |1     |\n",
      "|Ostby, Mr. Engelhart Cornelius                     |55         |61.9792 |1     |\n",
      "|Goodwin, Master. William Frederick                 |60         |46.9    |3     |\n",
      "|Icard, Miss. Amelie                                |62         |80.0    |1     |\n",
      "|Harris, Mr. Henry Birkhardt                        |63         |83.475  |1     |\n",
      "|Goodwin, Miss. Lillian Amy                         |72         |46.9    |3     |\n",
      "|Hood, Mr. Ambrose Jr                               |73         |73.5    |2     |\n",
      "|Bing, Mr. Lee                                      |75         |56.4958 |3     |\n",
      "|Carrau, Mr. Francisco M                            |84         |47.1    |1     |\n",
      "|Fortune, Miss. Mabel Helen                         |89         |263.0   |1     |\n",
      "|Chaffee, Mr. Herbert Fuller                        |93         |61.175  |1     |\n",
      "|Greenfield, Mr. William Bertram                    |98         |63.3583 |1     |\n",
      "+---------------------------------------------------+-----------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select('Name','PassengerID','Fare','Pclass').where(df['Fare']>40.50).show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window function example:-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from  pyspark.sql.functions import *\n",
    "\n",
    "#imprtant for performing window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| dept|salary|\n",
      "+---+-----+------+\n",
      "|  1|sales|  4500|\n",
      "|  2|sales|  5600|\n",
      "|  3|  dev|  4500|\n",
      "|  4|admin|  8500|\n",
      "|  5|admin|  8500|\n",
      "|  6|admin|  8500|\n",
      "|  7|admin|  8200|\n",
      "| 15|admin|  9200|\n",
      "|  8|  dev|  8500|\n",
      "|  9|  dev|  9500|\n",
      "| 10|sales|  9500|\n",
      "| 11|sales|  6100|\n",
      "| 12|sales|  4600|\n",
      "| 13|sales|  4000|\n",
      "| 14|sales|  8500|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lst = [(1,'sales',4500),\n",
    "(2,'sales',5600),\n",
    "(3,'dev',4500),\n",
    "(4,'admin',8500),\n",
    "(5,'admin',8500),\n",
    "(6,'admin',8500),\n",
    "(7,'admin',8200),\n",
    "(15,'admin',9200),\n",
    "(8,'dev',8500),\n",
    "(9,'dev',9500),\n",
    "(10,'sales',9500),\n",
    "(11,'sales',6100),\n",
    "(12,'sales',4600),\n",
    "(13,'sales',4000),\n",
    "(14,'sales',8500),\n",
    "]\n",
    "\n",
    "df4 = spark.createDataFrame(data= lst,schema= ['id','dept','salary'])\n",
    "df4.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "|id |dept |salary|row_number|\n",
      "+---+-----+------+----------+\n",
      "|7  |admin|8200  |1         |\n",
      "|4  |admin|8500  |2         |\n",
      "|5  |admin|8500  |3         |\n",
      "|6  |admin|8500  |4         |\n",
      "|15 |admin|9200  |5         |\n",
      "|3  |dev  |4500  |1         |\n",
      "|8  |dev  |8500  |2         |\n",
      "|9  |dev  |9500  |3         |\n",
      "|13 |sales|4000  |1         |\n",
      "|1  |sales|4500  |2         |\n",
      "|12 |sales|4600  |3         |\n",
      "|2  |sales|5600  |4         |\n",
      "|11 |sales|6100  |5         |\n",
      "|14 |sales|8500  |6         |\n",
      "|10 |sales|9500  |7         |\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "df4.withColumn(\"row_number\",row_number().over(windowSpec)).show(truncate=False)\n",
    "\n",
    "#windowing on the basis of the partioned column that is department and adding an extra column named as rown number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----+\n",
      "|id |dept |salary|rank|\n",
      "+---+-----+------+----+\n",
      "|7  |admin|8200  |1   |\n",
      "|4  |admin|8500  |2   |\n",
      "|5  |admin|8500  |2   |\n",
      "|6  |admin|8500  |2   |\n",
      "|15 |admin|9200  |5   |\n",
      "|3  |dev  |4500  |1   |\n",
      "|8  |dev  |8500  |2   |\n",
      "|9  |dev  |9500  |3   |\n",
      "|13 |sales|4000  |1   |\n",
      "|1  |sales|4500  |2   |\n",
      "|12 |sales|4600  |3   |\n",
      "|2  |sales|5600  |4   |\n",
      "|11 |sales|6100  |5   |\n",
      "|14 |sales|8500  |6   |\n",
      "|10 |sales|9500  |7   |\n",
      "+---+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"rank\",rank().over(windowSpec)).show(truncate=False)\n",
    "\n",
    "#rank() window function is used to provide a rank to the result within a window partition. \n",
    "# This function leaves gaps in rank when there are ties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------------+---------------+--------------+\n",
      "| id| dept|salary| salary list|Cummulative Sum|Average Salary|\n",
      "+---+-----+------+------------+---------------+--------------+\n",
      "|  7|admin|  8200|      [8200]|           8200|        8200.0|\n",
      "|  4|admin|  8500|[8200, 8500]|          16700|        8350.0|\n",
      "|  5|admin|  8500|[8500, 8500]|          17000|        8500.0|\n",
      "|  6|admin|  8500|[8500, 8500]|          17000|        8500.0|\n",
      "| 15|admin|  9200|[8500, 9200]|          17700|        8850.0|\n",
      "|  3|  dev|  4500|      [4500]|           4500|        4500.0|\n",
      "|  8|  dev|  8500|[4500, 8500]|          13000|        6500.0|\n",
      "|  9|  dev|  9500|[8500, 9500]|          18000|        9000.0|\n",
      "| 13|sales|  4000|      [4000]|           4000|        4000.0|\n",
      "|  1|sales|  4500|[4000, 4500]|           8500|        4250.0|\n",
      "| 12|sales|  4600|[4500, 4600]|           9100|        4550.0|\n",
      "|  2|sales|  5600|[4600, 5600]|          10200|        5100.0|\n",
      "| 11|sales|  6100|[5600, 6100]|          11700|        5850.0|\n",
      "| 14|sales|  8500|[6100, 8500]|          14600|        7300.0|\n",
      "| 10|sales|  9500|[8500, 9500]|          18000|        9000.0|\n",
      "+---+-----+------+------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"dept\").orderBy(\"salary\").rowsBetween(-1,Window.currentRow)\n",
    "\n",
    "#here we are specifying the amount of rows we want to be seen in the salary list portion.\n",
    "\n",
    "df4.withColumn(\"salary list\",collect_list('salary').over(windowSpec))\\\n",
    "    .withColumn(\"Cummulative Sum\",sum('salary').over(windowSpec))\\\n",
    "    .withColumn(\"Average Salary\",avg('salary').over(windowSpec)).show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
